# PageRank using spark

import org.apache.spark.HashPartitioner

val lines = sc.textFile("/home/leo/spark/web-Google/first1m.txt")

val links = lines.map{ s =>
      val parts = s.split("\\s+")
      (parts(0), parts(1))
}.distinct().groupByKey().partitionBy(new HashPartitioner(4)).persist()

var ranks = links.mapValues(v => 1.0)

val iters = 100

for (i <- 1 to iters) {
      val contribs = links.join(ranks).values.flatMap{ case (urls, rank) =>
        val size = urls.size
        urls.map(url => (url, rank / size))
      }
      ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)
    }

//val output = ranks.collect()
ranks.saveAsTextFile("/home/leo/spark/web-Google/ranks")
//output.foreach(tup => println(tup._1 + " has rank: " + tup._2 + "."))

val contribs = link2.join(rank).flatMap { case (pageId,(pageLinks,rank)) => pageLinks.map(dest => (dest, rank/pageLinks.size)) }

